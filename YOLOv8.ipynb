{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krupaltisgaonkar/pytorch-yolo/blob/main/YOLOv8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5iOQzj5C6C_"
      },
      "source": [
        "# **YOLOv8 Training**\n",
        "**Author:** Krupal Tisgaonkar\n",
        "\n",
        "**Last updated:** January 18, 2025\n",
        "\n",
        "---\n",
        "\n",
        "This notebook helps you:\n",
        "1. Upload or import a dataset from Google Drive.\n",
        "2. Resize images and labels for YOLO format\n",
        "3. Train a YOLOv8 model using the processed dataset.\n",
        "4. Save and download the trained model.\n",
        "5. You will be able to deploy on a local machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm1ykK1XC6DC"
      },
      "source": [
        "## **Step 1: Setup Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmfQ4ajgC6DC"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install ultralytics\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs(\"dataset\", exist_ok=True)\n",
        "os.makedirs(\"processed_dataset\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hgR7xPiC6DE"
      },
      "source": [
        "## **Step 2: Upload or Import Dataset**\n",
        "\n",
        "1. Upload a zipped dataset manually.\n",
        "2. Provide the path to a zipped dataset stored in Google Drive.\n",
        "\n",
        "Structure your dataset as follows:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "├── images/\n",
        "│   ├── train/\n",
        "│   ├── val/\n",
        "├── labels/\n",
        "│   ├── train/\n",
        "│   ├── val/\n",
        "```\n",
        "\n",
        "If your dataset isn't set like that, after this step, do Step A, otherwise don't do that step\n",
        "\n",
        "> **NOTE**: Make sure to name your dataset dataset.zip\n",
        "\n",
        "**Option 1: Upload zipped dataset manually**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ODrnHmzC6DF"
      },
      "outputs": [],
      "source": [
        "print(\"Upload your zipped dataset...\")\n",
        "uploaded = files.upload()\n",
        "# Extract the uploaded dataset\n",
        "if uploaded:\n",
        "    for filename in uploaded.keys():\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"custom_data\")\n",
        "        print(f\"Dataset extracted to: custom_data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkNW-PgeC6DF"
      },
      "source": [
        "**Option 2: Import from Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O4rZf7ZsC6DF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d71029-e986-4bee-d08b-8705b2eb08ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted from Google Drive to: custom_data/\n"
          ]
        }
      ],
      "source": [
        "drive_dataset_path = \"/content/drive/MyDrive/dataset/YOLO/dataset.zip\"  # Replace with your Google Drive dataset path\n",
        "if os.path.exists(drive_dataset_path):\n",
        "    with zipfile.ZipFile(drive_dataset_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"custom_data\")\n",
        "    print(f\"Dataset extracted from Google Drive to: custom_data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qORtkMttC6DG"
      },
      "source": [
        "### Step A: Make Dataset Compatible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G7SFsIbC6DG"
      },
      "outputs": [],
      "source": [
        "!wget -O /content/train_val_split.py https://raw.githubusercontent.com/EdjeElectronics/Train-and-Deploy-YOLO-Models/refs/heads/main/utils/train_val_split.py\n",
        "\n",
        "# TO DO: Improve robustness of train_val_split.py script so it can handle nested data folders, etc\n",
        "!python train_val_split.py --datapath=\"/content/custom_data\" --train_pct=0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-1Um30bC6DH"
      },
      "source": [
        "## **Step 3: Resize Images to 640x640**\n",
        "\n",
        "Resize all images in the `train` and `val` directories to `640x640`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uaNLje9mC6DH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1586be7d-b528-4e89-ec5f-77261aa004d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images resized and saved to processed_dataset/train/images.\n",
            "Images resized and saved to processed_dataset/val/images.\n"
          ]
        }
      ],
      "source": [
        "# Function to resize images\n",
        "def resize_images(input_dir, output_dir, new_size=(640, 640)):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            img_path = os.path.join(input_dir, filename)\n",
        "            img = Image.open(img_path)\n",
        "            img_resized = img.resize(new_size)\n",
        "            img_resized.save(os.path.join(output_dir, filename))\n",
        "    print(f\"Images resized and saved to {output_dir}.\")\n",
        "\n",
        "# Resize train and val images\n",
        "resize_images(\"data/train/images\", \"processed_dataset/train/images\")\n",
        "resize_images(\"data/validation/images\", \"processed_dataset/val/images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twbDpuyWC6DH"
      },
      "source": [
        "## **Step 4: Resize Labels**\n",
        "\n",
        "Adjust YOLO-format labels to align with the new image size. For the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocPbbfL6C6DI"
      },
      "outputs": [],
      "source": [
        "!wget -O /content/resize_labels.py https://raw.githubusercontent.com/krupaltisgaonkar/pytorch/refs/heads/main/scripts/resize_labels.py\n",
        "\n",
        "!python resize_labels.py --input_label_dir data/train/labels \\\n",
        "                 --input_image_dir data/train/images \\\n",
        "                 --output_label_dir processed_dataset/train/labels \\\n",
        "                 --new_size 640\n",
        "!python resize_labels.py --input_label_dir data/validation/labels \\\n",
        "                 --input_image_dir data/validation/images \\\n",
        "                 --output_label_dir processed_dataset/val/labels/ \\\n",
        "                 --new_size 640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3ZTQkMbC6DI"
      },
      "source": [
        "## **Step 5: Train YOLOv8**\n",
        "\n",
        "Use the resized dataset to train a YOLOv8 model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!mkdir datasets\n",
        "!cp -r /content/processed_dataset /content/datasets"
      ],
      "metadata": {
        "id": "2S1wJ6LBIE3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a data.yaml file for YOLO"
      ],
      "metadata": {
        "id": "P98jMqmdHg_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPp_RnhiC6DI"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "def create_data_yaml(path_to_classes_txt, path_to_data_yaml):\n",
        "\n",
        "  # Read class.txt to get class names\n",
        "  if not os.path.exists(path_to_classes_txt):\n",
        "    print(f'classes.txt file not found! Please create a classes.txt labelmap and move it to {path_to_classes_txt}')\n",
        "    return\n",
        "  with open(path_to_classes_txt, 'r') as f:\n",
        "    classes = []\n",
        "    for line in f.readlines():\n",
        "      if len(line.strip()) == 0: continue\n",
        "      classes.append(line.strip())\n",
        "  number_of_classes = len(classes)\n",
        "\n",
        "  # Create data dictionary\n",
        "  data = {\n",
        "      'path': '/content/processed_dataset',\n",
        "      'train': 'train/images',\n",
        "      'val': 'val/images',\n",
        "      'nc': number_of_classes,\n",
        "      'names': classes\n",
        "  }\n",
        "\n",
        "  # Write data to YAML file\n",
        "  with open(path_to_data_yaml, 'w') as f:\n",
        "    yaml.dump(data, f, sort_keys=False)\n",
        "  print(f'Created config file at {path_to_data_yaml}')\n",
        "\n",
        "  return\n",
        "\n",
        "# Define path to classes.txt and run function\n",
        "path_to_classes_txt = '/content/data/classes.txt'\n",
        "path_to_data_yaml = '/content/data.yaml'\n",
        "\n",
        "create_data_yaml(\"custom_data/classes.txt\", \"/content/data.yaml\")\n",
        "\n",
        "print('\\nFile contents:\\n')\n",
        "!cat /content/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Training!\n",
        "\n",
        "\n",
        "\n",
        "> **NOTE:** Your model might need more epochs if it has less images\n",
        "\n",
        "After training this will already download the model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GNuPg9YtHoXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo detect train data=/content/data.yaml model=yolo11s.pt epochs=2500 imgsz=640 patience=2501\n",
        "\n",
        "# Create \"my_model\" folder to store model weights and train results\n",
        "!mkdir /content/fish_detect\n",
        "!cp /content/runs/detect/train/weights/best.pt /content/fish_detect/my_model.pt\n",
        "!cp -r /content/runs/detect/train /content/fish_detect\n",
        "\n",
        "# Zip into \"my_model.zip\"\n",
        "%cd fish_detect\n",
        "!zip /content/fish_detect.zip model.pt\n",
        "!zip -r /content/fish_detect.zip train\n",
        "%cd /content\n",
        "files.download('/content/fish_detect.zip')"
      ],
      "metadata": {
        "id": "uMvPjgD0Hp_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: **Deploy on Local Machine**"
      ],
      "metadata": {
        "id": "rL2Jnw5SyWyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Anaconda Prompt, or you can just create a virtual environment by yourself.\n",
        "\n",
        "Name it whatever you want I named mine `yolo-env1`. Also ensure it is python 3.12.\n",
        "\n",
        "For Anaconda, I used this command:\n",
        "\n",
        "```\n",
        "conda create --name yolo-env1 python=3.12 -y\n",
        "conda activate yolo-env1\n",
        "```\n",
        "Now install Ultralytics by using\n",
        "\n",
        "```\n",
        "pip install ultralytics\n",
        "```\n",
        "\n",
        "If you have a NVIDIA GPU, CUDA enabled on your local computer, use this command to install the packages\n",
        "\n",
        "```\n",
        "pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "```\n",
        "\n",
        "If you **DO NOT** have a GPU, or the proper GPU, use this command to download the CPU version:\n",
        "\n",
        "```\n",
        "pip install torch torchvision torchaudio\n",
        "```\n",
        "\n",
        "Now to run this model, you will have to download the code from my github page.\n",
        "\n",
        "You can use either:\n",
        "\n",
        "```\n",
        "curl -o model_main.py https://raw.githubusercontent.com/krupaltisgaonkar/pytorch/refs/heads/main/scripts/model_main.py\n",
        "```\n",
        "\n",
        "or\n",
        "\n",
        "```\n",
        "wget -O model_main.py https://raw.githubusercontent.com/krupaltisgaonkar/pytorch/refs/heads/main/scripts/model_main.py\n",
        "```\n",
        "\n",
        "**REMEMBER TO CHANGE YOUR CUSTOM MODELS VARIABLE'S PATHS TO THE PATHS OF YOUR MODELS!!! IN THE CODE**\n",
        "\n",
        "\n",
        "Run the command:\n",
        "\n",
        "Use a YOLO Predefined Model (v5, v8, v11) with Webcam\n",
        "\n",
        "```\n",
        "python model_main.py --model v8 --webcam 0\n",
        "```\n",
        "\n",
        "Use a YOLO Predefined Model with an Image or Video or Directory\n",
        "\n",
        "```bash\n",
        "python model_main.py --model v5 --input ./data/image.jpg\n",
        "python model_main.py --model v11 --input ./data/video.mp4\n",
        "python model_main.py --model v11 --input ./data/\n",
        "```\n",
        "Use your custom model\n",
        "```\n",
        "python model_main.py --model custom1 --input ./data/image.jpg\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "N5LuelDdycqp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Errors**\n",
        "\n",
        "If you have question, go to the github repository, and under issues, put an issue."
      ],
      "metadata": {
        "id": "KYtXhnxkUVLk"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}